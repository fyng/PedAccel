{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Metrics most correlated to SBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import sys\n",
    "sys.path.append(\"..\") #give this script access to all modules in parent directory\n",
    "from pathlib import Path\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "import tsfel\n",
    "from operator import itemgetter\n",
    "import os\n",
    "from Data_Cleaning import preprocess\n",
    "import Actigraph_Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import statistics\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal parameters\n",
    "freq = 100 #signal is 100hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load All Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to Load data of interest\n",
    "#data_dir = 'C:/Users/sidha/OneDrive/Sid Stuff/PROJECTS/iMEDS Design Team/Data Analysis/PedAccel/data_analysis/PythonPipeline/PatientData'\n",
    "\n",
    "data_dir = r'C:\\Users\\jakes\\Documents\\DT 6 Analysis\\PythonCode\\PedAccel\\data_analysis\\PythonPipeline\\PatientData'\n",
    "\n",
    "\n",
    "#set params\n",
    "slice_size_min = 10\n",
    "lead_time = 5\n",
    "window_size = 100 #100 is 1 second worth of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Change for data of interest\n",
    "#preprocess.load_and_segment_data(data_dir, slice_size_min, lead_time) #take sbs csv and accel gt3x to create a .mat file with vector magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Most correlated features for some signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:498: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.kurtosis(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:518: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.skew(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:498: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.kurtosis(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:518: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.skew(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:498: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.kurtosis(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:518: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.skew(signal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of x 2333\n",
      "[[1.82523000e+05]\n",
      " [1.39183090e+07]\n",
      " [1.15985908e+04]\n",
      " ...\n",
      " [5.17817848e+02]\n",
      " [6.49846260e+02]\n",
      " [0.00000000e+00]]\n",
      "df with SBS removed for analysis:\n",
      "   SBS  hr_feature_0_Absolute energy  hr_feature_0_Area under the curve  \\\n",
      "0   -1                    13918309.0                           182523.0   \n",
      "\n",
      "   hr_feature_0_Autocorrelation  hr_feature_0_Average power  \\\n",
      "0                    13918309.0                11598.590833   \n",
      "\n",
      "   hr_feature_0_Centroid  hr_feature_0_ECDF Percentile Count_0  \\\n",
      "0             604.657721                                 120.0   \n",
      "\n",
      "   hr_feature_0_ECDF Percentile Count_1  hr_feature_0_ECDF Percentile_0  \\\n",
      "0                                 480.0                           148.0   \n",
      "\n",
      "   hr_feature_0_ECDF Percentile_1  ...  bps_feature_0_Wavelet variance_4  \\\n",
      "0                           156.0  ...                        392.833359   \n",
      "\n",
      "   bps_feature_0_Wavelet variance_4  bps_feature_0_Wavelet variance_5  \\\n",
      "0                        207.779628                        560.376151   \n",
      "\n",
      "   bps_feature_0_Wavelet variance_5  bps_feature_0_Wavelet variance_6  \\\n",
      "0                        296.397303                        756.008908   \n",
      "\n",
      "   bps_feature_0_Wavelet variance_6  bps_feature_0_Wavelet variance_7  \\\n",
      "0                         399.87248                        978.999369   \n",
      "\n",
      "   bps_feature_0_Wavelet variance_7  bps_feature_0_Wavelet variance_8  \\\n",
      "0                        517.817848                       1228.615586   \n",
      "\n",
      "   bps_feature_0_Wavelet variance_8  \n",
      "0                         649.84626  \n",
      "\n",
      "[1 rows x 3111 columns]\n",
      "x mean should be 0 but is really: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Perform PCA Using All Features\u001b[39;00m\n\u001b[0;32m    126\u001b[0m pca_actigraphy \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m principalComponents_actigraphy \u001b[38;5;241m=\u001b[39m \u001b[43mpca_actigraphy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_normalized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m principal_actigraphy_Df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mprincipalComponents_actigraphy,\n\u001b[0;32m    129\u001b[0m                                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprincipal component 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprincipal component 2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprincipal component 3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprincipal component 4\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExplained variation per principal component: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pca_actigraphy\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_))\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    279\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:483\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_array_api_compliant:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n\u001b[1;32m--> 483\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\sklearn\\utils\\validation.py:1003\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1000\u001b[0m     )\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1003\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "#There is no error handling in place, the .mat file must exist\n",
    "for patient in os.listdir(data_dir):\n",
    "        # filter out non-directories\n",
    "        patient_dir = os.path.join(data_dir, patient)\n",
    "        if os.path.isdir(patient_dir):\n",
    "            data_filepath_vitals = os.path.join(patient_dir, f'{patient}_SICKBAY_{lead_time}MIN_{slice_size_min - lead_time}MIN.mat')\n",
    "            data_filepath_accel = os.path.join(patient_dir, f'{patient}_{lead_time}MIN_{slice_size_min - lead_time}MIN.mat')           \n",
    "       \n",
    "        accel_data = loadmat(data_filepath_accel)\n",
    "        x_mag = accel_data[\"x_mag\"]\n",
    "        accel_SBS = accel_data[\"sbs\"]\n",
    "        \n",
    "        vitals_data = loadmat(data_filepath_vitals)\n",
    "        hr = vitals_data['heart_rate']\n",
    "        SpO2 = vitals_data['SpO2']\n",
    "        rr = vitals_data['respiratory_rate']\n",
    "        bps = vitals_data['blood_pressure_systolic']\n",
    "        bpm = vitals_data['blood_pressure_mean']\n",
    "        bpd = vitals_data['blood_pressure_diastolic']\n",
    "        vitals_SBS = vitals_data['sbs']\n",
    "\n",
    "        #Create a new cfg file for every metric to add to PCA\n",
    "        MAD_cfg_file = tsfel.get_features_by_domain()\n",
    "\n",
    "        hr_cfg_file = tsfel.get_features_by_domain()\n",
    "        SpO2_cfg_file = tsfel.get_features_by_domain()\n",
    "        rr_cfg_file = tsfel.get_features_by_domain()\n",
    "        bps_cfg_file = tsfel.get_features_by_domain()\n",
    "        bpm_cfg_file = tsfel.get_features_by_domain()\n",
    "        bpd_cfg_file = tsfel.get_features_by_domain()\n",
    "\n",
    "        # Extract features and restructure data\n",
    "        MAD_features_list = []\n",
    "        hr_features_list = []\n",
    "        rr_features_list = []\n",
    "        SpO2_features_list = []\n",
    "        bpd_features_list = []\n",
    "        bpm_features_list = []\n",
    "        bps_features_list = []\n",
    "        sbs_list = []\n",
    "\n",
    "        for i in range(len(vitals_SBS)):\n",
    "            #MAD = Actigraph_Metrics.VecMag_MAD(x_mag[i,:],100)\n",
    "\n",
    "            #Replace with HR, this is a test and uses 2 sets of MAD data instead of MAD and HR\n",
    "            \n",
    "            hr_features = tsfel.time_series_features_extractor(hr_cfg_file, hr[i], fs=.5, verbose=0)\n",
    "            SpO2_features = tsfel.time_series_features_extractor(SpO2_cfg_file, SpO2[i], fs=.5, verbose=0)\n",
    "            rr_features = tsfel.time_series_features_extractor(rr_cfg_file, rr[i], fs=.5, verbose=0)\n",
    "            bps_features = tsfel.time_series_features_extractor(bps_cfg_file, bps[i], fs=.5, verbose=0)\n",
    "            bpm_features = tsfel.time_series_features_extractor(bpm_cfg_file, bpm[i], fs=.5, verbose=0)\n",
    "            bpd_features = tsfel.time_series_features_extractor(bpd_cfg_file, bpd[i], fs=.5, verbose=0)\n",
    "\n",
    "            hr_features_list.append(hr_features)\n",
    "            SpO2_features_list.append(SpO2_features)\n",
    "            rr_features_list.append(rr_features)\n",
    "            bps_features_list.append(bps_features)\n",
    "            bpm_features_list.append(bpm_features)\n",
    "            bpd_features_list.append(bpd_features)\n",
    "\n",
    "            sbs_list.append(vitals_SBS[i])\n",
    "\n",
    "\n",
    "        #create these data frames for every new feature\n",
    "        hr_columns = ['hr_feature_' + str(col) for col in hr_features_list[0]]\n",
    "        hr_features_array = np.array(hr_features_list).reshape(-1, len(hr_columns))\n",
    "        hr_df_features = pd.DataFrame(hr_features_array)\n",
    "        hr_df_features.columns = hr_columns\n",
    "\n",
    "        SpO2_columns = ['SpO2_feature_' + str(col) for col in SpO2_features_list[0]]\n",
    "        SpO2_features_array = np.array(SpO2_features_list).reshape(-1, len(SpO2_columns))\n",
    "        SpO2_df_features = pd.DataFrame(SpO2_features_array)\n",
    "        SpO2_df_features.columns = SpO2_columns\n",
    "\n",
    "        rr_columns = ['rr_feature_' + str(col) for col in rr_features_list[0]]\n",
    "        rr_features_array = np.array(rr_features_list).reshape(-1, len(rr_columns))\n",
    "        rr_df_features = pd.DataFrame(rr_features_array)\n",
    "        rr_df_features.columns = rr_columns\n",
    "    \n",
    "        bps_columns = ['bps_feature_' + str(col) for col in bps_features_list[0]]\n",
    "        bps_features_array = np.array(bps_features_list).reshape(-1, len(bps_columns))\n",
    "        bps_df_features = pd.DataFrame(bps_features_array)\n",
    "        bps_df_features.columns = bps_columns\n",
    "    \n",
    "        bpm_columns = ['bpm_feature_' + str(col) for col in bpm_features_list[0]]\n",
    "        bpm_features_array = np.array(bpm_features_list).reshape(-1, len(bpm_columns))\n",
    "        bpm_df_features = pd.DataFrame(bpm_features_array)\n",
    "        bpm_df_features.columns = bpm_columns\n",
    "\n",
    "        bpd_columns = ['bps_feature_' + str(col) for col in bpd_features_list[0]]\n",
    "        bpd_features_array = np.array(bpd_features_list).reshape(-1, len(bpd_columns))\n",
    "        bpd_df_features = pd.DataFrame(bpd_features_array)\n",
    "        bpd_df_features.columns = bpd_columns\n",
    "\n",
    "        \n",
    "        df_sbs = pd.DataFrame({'SBS': sbs_list})\n",
    "        \n",
    "        #Concatenate all features and SBS scores\n",
    "        df_features = pd.concat([hr_df_features, SpO2_df_features, rr_df_features, bps_df_features, bpm_df_features, bpd_df_features], axis =1)\n",
    "        df = pd.concat([df_sbs, df_features], axis=1)\n",
    "        #Flatten arrays in DataFrame\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: x[0] if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "        # Calculate means for numeric columns\n",
    "        means = df.mean()\n",
    "\n",
    "        # Replace NaN values with column means\n",
    "        df_cleaned = df.fillna(means)\n",
    "        df = df_cleaned\n",
    "\n",
    "        x = df_features.iloc[:, 1:].values\n",
    "        x = x.T\n",
    "        print(f'length of x {len(x)}')\n",
    "        print(x)\n",
    "        print(\"df with SBS removed for analysis:\")\n",
    "        print(df[df.columns[:-1]].head(5))\n",
    "        y = df['SBS'].values   \n",
    "        \n",
    "        # Normalize features\n",
    "        features = np.hstack((hr_df_features.columns ,SpO2_df_features.columns,rr_df_features.columns,bps_df_features.columns,bpm_df_features.columns,bpd_df_features.columns))\n",
    "        x_normalized = StandardScaler().fit_transform(x)\n",
    "        print(f'x mean should be 0 but is really: {np.mean((x_normalized))}')\n",
    "        \n",
    "        # Perform PCA Using All Features\n",
    "        pca_actigraphy = PCA(n_components=4)\n",
    "        principalComponents_actigraphy = pca_actigraphy.fit_transform(x_normalized)\n",
    "        principal_actigraphy_Df = pd.DataFrame(data=principalComponents_actigraphy,\n",
    "                                            columns=['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])\n",
    "\n",
    "        print('Explained variation per principal component: {}'.format(pca_actigraphy.explained_variance_ratio_))\n",
    "\n",
    "        # Plot PCA for each principal component\n",
    "        for component in range(pca_actigraphy.n_components_ - 1):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.xlabel(f'Principal Component - {component+1}', fontsize=12)\n",
    "            plt.ylabel('Principal Component - {}'.format(component+2), fontsize=12)\n",
    "            plt.title(f'Principal Component Analysis of Actigraphy and SBS\\n for {patient}_{lead_time}MIN_{slice_size_min - lead_time}MIN with vitals data only', fontsize=14)\n",
    "            \n",
    "            for i in range(len(df_sbs)):\n",
    "                if df['SBS'][i] == -1:\n",
    "                    color = 'purple'\n",
    "                elif df['SBS'][i] == 0:\n",
    "                    color = 'blue'\n",
    "                elif df['SBS'][i] == 1:\n",
    "                    color = 'orange'\n",
    "                elif df['SBS'][i] == 2:\n",
    "                    color = 'red'\n",
    "                plt.scatter(principal_actigraphy_Df.loc[i, f'principal component {component+1}'], \n",
    "                            principal_actigraphy_Df.loc[i, f'principal component {component+2}'], \n",
    "                            c=color, s=50)\n",
    "            \n",
    "            # Manually create a legend\n",
    "            neg1 = mlines.Line2D([], [], color='purple', marker='o', ls='', label='SBS -1')\n",
    "            zero = mlines.Line2D([], [], color='blue', marker='o', ls='', label='SBS 0')\n",
    "            one = mlines.Line2D([], [], color='orange', marker='o', ls='', label='SBS 1')\n",
    "            two = mlines.Line2D([], [], color='red', marker='o', ls='', label='SBS 2')\n",
    "            plt.legend(handles=[neg1, zero, one, two])\n",
    "            \n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DT6Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

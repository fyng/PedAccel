{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") #give this script access to all modules in parent directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Import Statistical Tests and tsfel\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "import tsfel\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import statistics\n",
    "\n",
    "# Import Previous Scripts\n",
    "import Filtering\n",
    "import Correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr_vitals_sbs(signal, sbs, signal_name, lead_time, slice_size_min):\n",
    "    '''\n",
    "    @param signal: vitals signal input\n",
    "    @param sbs: sbs corresponding to vitals signal\n",
    "    @param signal_name: name of input signal\n",
    "    '''\n",
    "    # Assuming tsfel and other necessary imports are already done\n",
    "\n",
    "    cfg_file = tsfel.get_features_by_domain()\n",
    "    features_list = []\n",
    "    sbs_list = []\n",
    "    fs = .5\n",
    "    \n",
    "    # Assuming signal and sbs are lists\n",
    "    for i in range(len(signal)):\n",
    "        sbs_list.append(sbs[i])\n",
    "        features = tsfel.time_series_features_extractor(cfg_file, signal[i], fs, verbose=0)\n",
    "        features_list.append(features)\n",
    "\n",
    "    # print(features_list)\n",
    "    columns = [col for col in list(features_list[0])]\n",
    "    features_array = np.array(features_list).reshape(-1, 359)\n",
    "    # print(features_array)\n",
    "    df_features = pd.DataFrame(features_array)\n",
    "    \n",
    "    #Pearson Correlation Coefficient\n",
    "    my_dict = {}\n",
    "    df_features.columns = columns\n",
    "    for i in columns:\n",
    "        y = sbs_list\n",
    "        x = list(df_features[i])\n",
    "        if len(y) >= 2 and len(x) >= 2:\n",
    "            corr, _ = pearsonr(y, x)\n",
    "            my_dict[i] = np.abs(corr)\n",
    "        else:\n",
    "            my_dict[i] = np.nan\n",
    "\n",
    "    # Filter out NaN values from the dictionary\n",
    "    clean_dict = {k: v for k, v in my_dict.items() if not np.isnan(v)}\n",
    "\n",
    "    # Retrieve N features with best correlation coefficient  \n",
    "    # Initialize N\n",
    "    N = 5\n",
    "    \n",
    "    # N largest values in dictionary\n",
    "    # Using sorted() + itemgetter() + items()\n",
    "    res = dict(sorted(clean_dict.items(), key=itemgetter(1), reverse=True)[:N])\n",
    "\n",
    "    # Plotting\n",
    "    y = list(res.keys())\n",
    "    x = list(res.values())\n",
    "    \n",
    "    # Figure Size\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Horizontal Bar Plot\n",
    "    ax.barh(y, x)\n",
    "    \n",
    "    # Remove axes splines\n",
    "    for s in ['top', 'bottom', 'left', 'right']:\n",
    "        ax.spines[s].set_visible(False)\n",
    "    \n",
    "    # Remove x, y Ticks\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    \n",
    "    # Add padding between axes and labels\n",
    "    ax.xaxis.set_tick_params(pad=5)\n",
    "    ax.yaxis.set_tick_params(pad=10)\n",
    "    \n",
    "    # Add x, y gridlines\n",
    "    ax.grid(color='grey', linestyle='-.', linewidth=0.5, alpha=0.2)\n",
    "    \n",
    "    # Show top values \n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Set x axis range\n",
    "    ax.set_xlim([0.8 * min(x), 1.1 * max(x)])\n",
    "    \n",
    "    # Add Plot Title\n",
    "    ax.set_title(f'Correlation between top features and SBS for\\n {signal_name}_{lead_time}MIN_{slice_size_min - lead_time}MIN)',\n",
    "                            loc ='left', )\n",
    "    \n",
    "    # Show Plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the top N correlated features\n",
    "    return \"The top N value pairs are \" + str(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "data_dir = 'C:/Users/sidha/OneDrive/Sid Stuff/PROJECTS/iMEDS Design Team/Data Analysis/PedAccel/data_analysis/PythonPipeline/PatientData'\n",
    "# data_dir = r'C:\\Users\\jakes\\Documents\\DT 6 Analysis\\PythonCode\\PedAccel\\data_analysis\\PythonPipeline\\PatientData'\n",
    "slice_size_min = 15\n",
    "lead_time = 10\n",
    "sr = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is no error handling in place, the .mat file must exist\n",
    "for patient in os.listdir(data_dir):\n",
    "    # filter out non-directories\n",
    "    print(f\"Processing{patient}\")\n",
    "    patient_dir = os.path.join(data_dir, patient)\n",
    "    if os.path.isdir(patient_dir):\n",
    "       # data_filepath_accel = os.path.join(patient_dir, f'{patient}_{lead_time}MIN_{slice_size_min - lead_time}MIN.mat')           \n",
    "        data_filepath_vitals = os.path.join(patient_dir, f'{patient}_SICKBAY_{lead_time}MIN_{slice_size_min - lead_time}MIN_Validated_Stim.mat')\n",
    "        \n",
    "       # accel_data = loadmat(data_filepath_accel)\n",
    "       # x_mag = accel_data[\"x_mag\"]\n",
    "       # accel_SBS = accel_data[\"sbs\"].flatten()\n",
    "        \n",
    "        vitals_data = loadmat(data_filepath_vitals)\n",
    "        temp_hr = vitals_data['heart_rate']\n",
    "        temp_SpO2 = vitals_data['SpO2']\n",
    "        temp_rr = vitals_data['respiratory_rate']\n",
    "        temp_bps = vitals_data['blood_pressure_systolic']\n",
    "        temp_bpm = vitals_data['blood_pressure_mean']\n",
    "        temp_bpd = vitals_data['blood_pressure_diastolic']\n",
    "        vitals_SBS = vitals_data['sbs'].flatten()\n",
    "        hr = []\n",
    "        rr = []\n",
    "        SpO2 = []\n",
    "        bpm = []\n",
    "        bps = []\n",
    "        bpd = []\n",
    "        vitals_list = [hr,rr,SpO2,bpm,bps,bpd]\n",
    "        vitals_names = ['hr','rr','spo2','bpm','bps','bpd']\n",
    "        temp_vitals = [temp_hr,temp_rr, temp_SpO2,temp_bpm,temp_bps,temp_bpd] \n",
    "        \n",
    "        flag_list = [0] * (int)(sr * 60 * slice_size_min) #generate a list to insert in place of invalid data, \n",
    "        #this list serves as a flag for a window to ignore in the box plot function\n",
    "        \n",
    "        \n",
    "        for j in range(len(vitals_list)): #go through every vitals metric\n",
    "            print(f'original {vitals_names[j]} vitals array shape: {np.array(temp_vitals[j]).shape} ')\n",
    "            for i in range(len(vitals_SBS)): #go through every SBS score for each vitals metric\n",
    "                if (Filtering.checkVitals(temp_vitals[j][i], slice_size_min, vitals_names[j])): #check the data in a single window\n",
    "                    vitals_list[j].append(temp_vitals[j][i]) #append that single window data to the 2D hr,rr,spo2,bpm,bps,bpd arrays if that window's data is valid\n",
    "                else:\n",
    "                    vitals_list[j].append(flag_list) #append an array of zeros for window number i for the jth vitals metric if the data is invalid(i.e. too many NaN points)\n",
    "                    print(f'{vitals_names[j]} SBS index {i} has insufficient data, zeros appended in place') \n",
    "            print(f'final {vitals_names[j]} vitals array shape: {np.array(vitals_list[j]).shape}') #should be the number of SBS scores by the number of samples in a window\n",
    "        \n",
    "        \n",
    "        \n",
    "        for signal, name in zip(vitals_list, vitals_names):\n",
    "            mean_val = np.mean(signal)\n",
    "            if len(signal) != 0:\n",
    "                if mean_val != 0:\n",
    "                    pearson_corr_vitals_sbs(signal, vitals_SBS, name, lead_time, slice_size_min)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

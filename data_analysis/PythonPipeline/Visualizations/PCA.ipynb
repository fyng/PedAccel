{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Metrics most correlated to SBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import sys\n",
    "sys.path.append(\"..\") #give this script access to all modules in parent directory\n",
    "from pathlib import Path\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "import tsfel\n",
    "from operator import itemgetter\n",
    "import os\n",
    "from Data_Cleaning import preprocess\n",
    "import Actigraph_Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import statistics\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal parameters\n",
    "freq = 100 #signal is 100hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load All Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to Load data of interest\n",
    "#data_dir = 'C:/Users/sidha/OneDrive/Sid Stuff/PROJECTS/iMEDS Design Team/Data Analysis/PedAccel/data_analysis/PythonPipeline/PatientData'\n",
    "\n",
    "data_dir = r'C:\\Users\\jakes\\Documents\\DT 6 Analysis\\PythonCode\\PedAccel\\data_analysis\\PythonPipeline\\PatientData'\n",
    "\n",
    "\n",
    "#set params\n",
    "slice_size_min = 10\n",
    "lead_time = 5\n",
    "window_size = 100 #100 is 1 second worth of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Change for data of interest\n",
    "#preprocess.load_and_segment_data(data_dir, slice_size_min, lead_time) #take sbs csv and accel gt3x to create a .mat file with vector magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Most correlated features for some signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:498: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.kurtosis(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:518: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.skew(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:498: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.kurtosis(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:518: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.skew(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:498: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.kurtosis(signal)\n",
      "c:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\tsfel\\feature_extraction\\features.py:518: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return scipy.stats.skew(signal)\n",
      "C:\\Users\\jakes\\AppData\\Local\\Temp\\ipykernel_36164\\1564966803.py:104: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated = df.interpolate()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m df_interpolated \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39minterpolate()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Remove NaN values from DataFrame.values\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m cleaned_values \u001b[38;5;241m=\u001b[39m df_interpolated\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m~\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_interpolated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    108\u001b[0m x \u001b[38;5;241m=\u001b[39m cleaned_values\n\u001b[0;32m    109\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSBS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues   \n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "#There is no error handling in place, the .mat file must exist\n",
    "for patient in os.listdir(data_dir):\n",
    "        # filter out non-directories\n",
    "        patient_dir = os.path.join(data_dir, patient)\n",
    "        if os.path.isdir(patient_dir):\n",
    "            data_filepath_vitals = os.path.join(patient_dir, f'{patient}_SICKBAY_{lead_time}MIN_{slice_size_min - lead_time}MIN.mat')\n",
    "            data_filepath_accel = os.path.join(patient_dir, f'{patient}_{lead_time}MIN_{slice_size_min - lead_time}MIN.mat')           \n",
    "       \n",
    "        accel_data = loadmat(data_filepath_accel)\n",
    "        x_mag = accel_data[\"x_mag\"]\n",
    "        accel_SBS = accel_data[\"sbs\"]\n",
    "        \n",
    "        vitals_data = loadmat(data_filepath_vitals)\n",
    "        hr = vitals_data['heart_rate']\n",
    "        SpO2 = vitals_data['SpO2']\n",
    "        rr = vitals_data['respiratory_rate']\n",
    "        bps = vitals_data['blood_pressure_systolic']\n",
    "        bpm = vitals_data['blood_pressure_mean']\n",
    "        bpd = vitals_data['blood_pressure_diastolic']\n",
    "        vitals_SBS = vitals_data['sbs']\n",
    "\n",
    "        #Create a new cfg file for every metric to add to PCA\n",
    "        MAD_cfg_file = tsfel.get_features_by_domain()\n",
    "\n",
    "        hr_cfg_file = tsfel.get_features_by_domain()\n",
    "        SpO2_cfg_file = tsfel.get_features_by_domain()\n",
    "        rr_cfg_file = tsfel.get_features_by_domain()\n",
    "        bps_cfg_file = tsfel.get_features_by_domain()\n",
    "        bpm_cfg_file = tsfel.get_features_by_domain()\n",
    "        bpd_cfg_file = tsfel.get_features_by_domain()\n",
    "\n",
    "        # Extract features and restructure data\n",
    "        MAD_features_list = []\n",
    "        hr_features_list = []\n",
    "        rr_features_list = []\n",
    "        SpO2_features_list = []\n",
    "        bpd_features_list = []\n",
    "        bpm_features_list = []\n",
    "        bps_features_list = []\n",
    "        sbs_list = []\n",
    "\n",
    "        for i in range(len(vitals_SBS)):\n",
    "            #MAD = Actigraph_Metrics.VecMag_MAD(x_mag[i,:],100)\n",
    "\n",
    "            #Replace with HR, this is a test and uses 2 sets of MAD data instead of MAD and HR\n",
    "            \n",
    "            hr_features = tsfel.time_series_features_extractor(hr_cfg_file, hr[i], fs=.5, verbose=0)\n",
    "            SpO2_features = tsfel.time_series_features_extractor(SpO2_cfg_file, SpO2[i], fs=.5, verbose=0)\n",
    "            rr_features = tsfel.time_series_features_extractor(rr_cfg_file, rr[i], fs=.5, verbose=0)\n",
    "            bps_features = tsfel.time_series_features_extractor(bps_cfg_file, bps[i], fs=.5, verbose=0)\n",
    "            bpm_features = tsfel.time_series_features_extractor(bpm_cfg_file, bpm[i], fs=.5, verbose=0)\n",
    "            bpd_features = tsfel.time_series_features_extractor(bpd_cfg_file, bpd[i], fs=.5, verbose=0)\n",
    "\n",
    "            hr_features_list.append(hr_features)\n",
    "            SpO2_features_list.append(SpO2_features)\n",
    "            rr_features_list.append(rr_features)\n",
    "            bps_features_list.append(bps_features)\n",
    "            bpm_features_list.append(bpm_features)\n",
    "            bpd_features_list.append(bpd_features)\n",
    "\n",
    "            sbs_list.append(vitals_SBS[i])\n",
    "\n",
    "\n",
    "        #create these data frames for every new feature\n",
    "        hr_columns = ['hr_feature_' + str(col) for col in hr_features_list[0]]\n",
    "        hr_features_array = np.array(hr_features_list).reshape(-1, len(hr_columns))\n",
    "        hr_df_features = pd.DataFrame(hr_features_array)\n",
    "        hr_df_features.columns = hr_columns\n",
    "\n",
    "        SpO2_columns = ['SpO2_feature_' + str(col) for col in SpO2_features_list[0]]\n",
    "        SpO2_features_array = np.array(SpO2_features_list).reshape(-1, len(SpO2_columns))\n",
    "        SpO2_df_features = pd.DataFrame(SpO2_features_array)\n",
    "        SpO2_df_features.columns = SpO2_columns\n",
    "\n",
    "        rr_columns = ['rr_feature_' + str(col) for col in rr_features_list[0]]\n",
    "        rr_features_array = np.array(rr_features_list).reshape(-1, len(rr_columns))\n",
    "        rr_df_features = pd.DataFrame(rr_features_array)\n",
    "        rr_df_features.columns = rr_columns\n",
    "    \n",
    "        bps_columns = ['bps_feature_' + str(col) for col in bps_features_list[0]]\n",
    "        bps_features_array = np.array(bps_features_list).reshape(-1, len(bps_columns))\n",
    "        bps_df_features = pd.DataFrame(bps_features_array)\n",
    "        bps_df_features.columns = bps_columns\n",
    "    \n",
    "        bpm_columns = ['bpm_feature_' + str(col) for col in bpm_features_list[0]]\n",
    "        bpm_features_array = np.array(bpm_features_list).reshape(-1, len(bpm_columns))\n",
    "        bpm_df_features = pd.DataFrame(bpm_features_array)\n",
    "        bpm_df_features.columns = bpm_columns\n",
    "\n",
    "        bpd_columns = ['bps_feature_' + str(col) for col in bpd_features_list[0]]\n",
    "        bpd_features_array = np.array(bpd_features_list).reshape(-1, len(bpd_columns))\n",
    "        bpd_df_features = pd.DataFrame(bpd_features_array)\n",
    "        bpd_df_features.columns = bpd_columns\n",
    "\n",
    "        \n",
    "        df_sbs = pd.DataFrame({'SBS': sbs_list})\n",
    "        \n",
    "        #Concatenate all features and SBS scores\n",
    "        df_features = pd.concat([hr_df_features, SpO2_df_features, rr_df_features, bps_df_features, bpm_df_features, bpd_df_features], axis =1)\n",
    "        df = pd.concat([df_sbs, df_features], axis=1)\n",
    "\n",
    "        #remove NaN values\n",
    "        # Interpolate NaN values\n",
    "        df_interpolated = df.interpolate()\n",
    "\n",
    "        # Remove NaN values from DataFrame.values\n",
    "        cleaned_values = df_interpolated.values[~np.isnan(df_interpolated.values).any(axis=1)]\n",
    "        x = cleaned_values\n",
    "        y = df['SBS'].values   \n",
    "        \n",
    "        # Normalize features\n",
    "        features = np.hstack((hr_df_features.columns ,SpO2_df_features.columns,rr_df_features.columns,bps_df_features.columns,bpm_df_features.columns,bpd_df_features.columns))\n",
    "        x = df_features.iloc[:, 1:].values\n",
    "        x_normalized = StandardScaler().fit_transform(x)\n",
    "        print(x)\n",
    "        print(f'x mean should be 0 but is really: {np.mean((x_normalized))}')\n",
    "        \n",
    "        # Perform PCA Using All Features\n",
    "        pca_actigraphy = PCA(n_components=4)\n",
    "        principalComponents_actigraphy = pca_actigraphy.fit_transform(x_normalized)\n",
    "        principal_actigraphy_Df = pd.DataFrame(data=principalComponents_actigraphy,\n",
    "                                            columns=['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])\n",
    "\n",
    "        print('Explained variation per principal component: {}'.format(pca_actigraphy.explained_variance_ratio_))\n",
    "\n",
    "        # Plot PCA for each principal component\n",
    "        for component in range(pca_actigraphy.n_components_ - 1):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.xlabel(f'Principal Component - {component+1}', fontsize=12)\n",
    "            plt.ylabel('Principal Component - {}'.format(component+2), fontsize=12)\n",
    "            plt.title(f'Principal Component Analysis of Actigraphy and SBS\\n for {patient}_{lead_time}MIN_{slice_size_min - lead_time}MIN with vitals data only', fontsize=14)\n",
    "            \n",
    "            for i in range(len(df_sbs)):\n",
    "                if df['SBS'][i] == -1:\n",
    "                    color = 'purple'\n",
    "                elif df['SBS'][i] == 0:\n",
    "                    color = 'blue'\n",
    "                elif df['SBS'][i] == 1:\n",
    "                    color = 'orange'\n",
    "                elif df['SBS'][i] == 2:\n",
    "                    color = 'red'\n",
    "                plt.scatter(principal_actigraphy_Df.loc[i, f'principal component {component+1}'], \n",
    "                            principal_actigraphy_Df.loc[i, f'principal component {component+2}'], \n",
    "                            c=color, s=50)\n",
    "            \n",
    "            # Manually create a legend\n",
    "            neg1 = mlines.Line2D([], [], color='purple', marker='o', ls='', label='SBS -1')\n",
    "            zero = mlines.Line2D([], [], color='blue', marker='o', ls='', label='SBS 0')\n",
    "            one = mlines.Line2D([], [], color='orange', marker='o', ls='', label='SBS 1')\n",
    "            two = mlines.Line2D([], [], color='red', marker='o', ls='', label='SBS 2')\n",
    "            plt.legend(handles=[neg1, zero, one, two])\n",
    "            \n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DT6Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

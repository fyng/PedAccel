{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vitals Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakes\\AppData\\Local\\Temp\\ipykernel_2728\\1887116167.py:9: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") #give this script access to all modules in parent directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Import Statistical Tests and tsfel\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "import tsfel\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import statistics\n",
    "\n",
    "# Import Previous Scripts\n",
    "import Filtering\n",
    "import Correlations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Function using TSFEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr_vitals_sbs(signal, sbs, signal_name, lead_time, slice_size_min):\n",
    "    '''\n",
    "    @param signal: vitals signal input\n",
    "    @param sbs: sbs corresponding to vitals signal\n",
    "    @param signal_name: name of input signal\n",
    "    '''\n",
    "    # Assuming tsfel and other necessary imports are already done\n",
    "\n",
    "    cfg_file = tsfel.get_features_by_domain()\n",
    "    features_list = []\n",
    "    sbs_list = []\n",
    "    fs = .5\n",
    "    \n",
    "    # Assuming signal and sbs are lists\n",
    "    for i in range(len(signal)):\n",
    "        if np.mean(np.array(signal[i])) != 0: #check for flag list\n",
    "            sbs_list.append(sbs[i])\n",
    "            features = tsfel.time_series_features_extractor(cfg_file, signal[i], fs, verbose=0)\n",
    "            features_list.append(features) #vertically concatenate features data frame\n",
    "            print(features)\n",
    "        else: \n",
    "            print(f'flag list detected, sbs at index {i} ignores')\n",
    "\n",
    "    print(len(features_list[0]))\n",
    "\n",
    "    #list comprehension for column names\n",
    "     columns = [col for col in list(features_list[0])]\n",
    "    # Convert features and SBS scores to DataFrame\n",
    "    features_array = np.array(features_list).reshape(-1, len(columns)) #may need to change 389\n",
    "    df_features = pd.DataFrame(features_array)\n",
    "\n",
    "    df_features.columns = columns\n",
    "\n",
    "    #Pearson Correlation Coefficient\n",
    "    CCoeff = []\n",
    "    for i in columns:\n",
    "        y = sbs_list\n",
    "        myX = list(df_features[i])\n",
    "        nan_indices = [i for i, x in enumerate(myX) if math.isnan(x)]\n",
    "        myX = [x for x in myX if not math.isnan(x)]\n",
    "        cleaned_y = [val for idx, val in enumerate(y) if idx not in nan_indices]\n",
    "\n",
    "        corr, _ = pearsonr(cleaned_y, myX)\n",
    "        CCoeff.append(np.abs(corr))\n",
    "        my_dict = dict(zip(list(columns), list(CCoeff)))\n",
    "\n",
    "    # functional\n",
    "    clean_dict = filter(lambda k: not math.isnan(my_dict[k]), my_dict)\n",
    "    # dict comprehension\n",
    "    clean_dict = {k: my_dict[k] for k in my_dict if not math.isnan(my_dict[k])}\n",
    "\n",
    "    #Retrieve N features with best correlation coefficient  \n",
    "    # Initialize N\n",
    "     N = 5\n",
    "            \n",
    "    # N largest values in dictionary\n",
    "    # Using sorted() + itemgetter() + items()\n",
    "    res = dict(sorted(clean_dict.items(), key=itemgetter(1), reverse=True)[:N])\n",
    "\n",
    "    # printing result\n",
    "    print(\"The top N value pairs are \" + str(res))\n",
    "\n",
    "    #Plot a histogram\n",
    "    y = list(res.keys())\n",
    "    x = list(res.values()) #price\n",
    "    \n",
    "    if len(x) != 0:\n",
    "        # Figure Size\n",
    "        fig, ax = plt.subplots(figsize =(10 ,5))\n",
    "\n",
    "        # Horizontal Bar Plot\n",
    "        ax.barh(y, x)\n",
    "        \n",
    "        # Remove axes splines\n",
    "        for s in ['top', 'bottom', 'left', 'right']:\n",
    "            ax.spines[s].set_visible(False)\n",
    "        \n",
    "        # Remove x, y Ticks\n",
    "        ax.xaxis.set_ticks_position('none')\n",
    "        ax.yaxis.set_ticks_position('none')\n",
    "        \n",
    "        # Add padding between axes and labels\n",
    "        ax.xaxis.set_tick_params(pad = 5)\n",
    "        ax.yaxis.set_tick_params(pad = 10)\n",
    "        \n",
    "        # Add x, y gridlines\n",
    "        ax.grid(color ='grey',\n",
    "                linestyle ='-.', linewidth = 0.5,\n",
    "                alpha = 0.2)\n",
    "\n",
    "        # Show top values \n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        #set x axis range\n",
    "        ax.set_xlim([.8*min(x),1.1*max(x)])\n",
    "        # Add Plot Title\n",
    "        ax.set_title(f'Correlation between top features and SBS for\\n {patient}_{lead_time}MIN_{slice_size_min - lead_time}MIN {signal_names[count]})',\n",
    "                            loc ='left', )\n",
    "                \n",
    "        # Show Plot\n",
    "\n",
    "        plt.show()\n",
    "        count= count+1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "# data_dir = 'C:/Users/sidha/OneDrive/Sid Stuff/PROJECTS/iMEDS Design Team/Data Analysis/PedAccel/data_analysis/PythonPipeline/PatientData'\n",
    "data_dir = r'C:\\Users\\jakes\\Documents\\DT 6 Analysis\\PythonCode\\PedAccel\\data_analysis\\PythonPipeline\\PatientData'\n",
    "slice_size_min = 15\n",
    "lead_time = 10\n",
    "sr = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingPatient11\n",
      "original hr vitals array shape: (14, 450) \n",
      "final hr vitals array shape: (14, 450)\n",
      "original rr vitals array shape: (14, 450) \n",
      "rr SBS index 0 has insufficient data, zeros appended in place\n",
      "rr SBS index 1 has insufficient data, zeros appended in place\n",
      "rr SBS index 2 has insufficient data, zeros appended in place\n",
      "rr SBS index 3 has insufficient data, zeros appended in place\n",
      "rr SBS index 4 has insufficient data, zeros appended in place\n",
      "rr SBS index 5 has insufficient data, zeros appended in place\n",
      "rr SBS index 6 has insufficient data, zeros appended in place\n",
      "rr SBS index 7 has insufficient data, zeros appended in place\n",
      "rr SBS index 8 has insufficient data, zeros appended in place\n",
      "rr SBS index 9 has insufficient data, zeros appended in place\n",
      "rr SBS index 10 has insufficient data, zeros appended in place\n",
      "rr SBS index 11 has insufficient data, zeros appended in place\n",
      "rr SBS index 12 has insufficient data, zeros appended in place\n",
      "rr SBS index 13 has insufficient data, zeros appended in place\n",
      "final rr vitals array shape: (14, 450)\n",
      "original spo2 vitals array shape: (14, 450) \n",
      "final spo2 vitals array shape: (14, 450)\n",
      "original bpm vitals array shape: (14, 450) \n",
      "final bpm vitals array shape: (14, 450)\n",
      "original bps vitals array shape: (14, 450) \n",
      "final bps vitals array shape: (14, 450)\n",
      "original bpd vitals array shape: (14, 450) \n",
      "final bpd vitals array shape: (14, 450)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m signal, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(vitals_list, vitals_names): \u001b[38;5;66;03m#2D array for each vitals is input to function\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(signal)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m         \u001b[43mpearson_corr_vitals_sbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvitals_SBS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_size_min\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 19\u001b[0m, in \u001b[0;36mpearson_corr_vitals_sbs\u001b[1;34m(signal, sbs, signal_name, lead_time, slice_size_min)\u001b[0m\n\u001b[0;32m     17\u001b[0m sbs_list\u001b[38;5;241m.\u001b[39mappend(sbs[i])\n\u001b[0;32m     18\u001b[0m features \u001b[38;5;241m=\u001b[39m tsfel\u001b[38;5;241m.\u001b[39mtime_series_features_extractor(cfg_file, signal[i], fs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(features))\n\u001b[0;32m     21\u001b[0m features_list\u001b[38;5;241m.\u001b[39mappend(features) \u001b[38;5;66;03m#vertically concatenate features data frame\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\jakes\\.virtualenvs\\DT6Analysis\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3807\u001b[0m     ):\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "#There is no error handling in place, the .mat file must exist\n",
    "for patient in os.listdir(data_dir):\n",
    "    # filter out non-directories\n",
    "    print(f\"Processing{patient}\")\n",
    "    patient_dir = os.path.join(data_dir, patient)\n",
    "    if os.path.isdir(patient_dir):\n",
    "       # data_filepath_accel = os.path.join(patient_dir, f'{patient}_{lead_time}MIN_{slice_size_min - lead_time}MIN.mat')           \n",
    "        data_filepath_vitals = os.path.join(patient_dir, f'{patient}_SICKBAY_{slice_size_min - lead_time}MIN_{lead_time}MIN.mat')\n",
    "        \n",
    "       # accel_data = loadmat(data_filepath_accel)\n",
    "       # x_mag = accel_data[\"x_mag\"]\n",
    "       # accel_SBS = accel_data[\"sbs\"].flatten()\n",
    "        \n",
    "        vitals_data = loadmat(data_filepath_vitals)\n",
    "        temp_hr = vitals_data['heart_rate']\n",
    "        temp_SpO2 = vitals_data['SpO2']\n",
    "        temp_rr = vitals_data['respiratory_rate']\n",
    "        temp_bps = vitals_data['blood_pressure_systolic']\n",
    "        temp_bpm = vitals_data['blood_pressure_mean']\n",
    "        temp_bpd = vitals_data['blood_pressure_diastolic']\n",
    "        vitals_SBS = vitals_data['sbs'].flatten()\n",
    "        hr = []\n",
    "        rr = []\n",
    "        SpO2 = []\n",
    "        bpm = []\n",
    "        bps = []\n",
    "        bpd = []\n",
    "        vitals_list = [hr,rr,SpO2,bpm,bps,bpd]\n",
    "        vitals_names = ['hr','rr','spo2','bpm','bps','bpd']\n",
    "        temp_vitals = [temp_hr,temp_rr, temp_SpO2,temp_bpm,temp_bps,temp_bpd] \n",
    "        \n",
    "        flag_list = [0] * (int)(sr * 60 * slice_size_min) #generate a list to insert in place of invalid data, \n",
    "        #this list serves as a flag for a window to ignore in the box plot function\n",
    "        \n",
    "        \n",
    "        for j in range(len(vitals_list)): #go through every vitals metric\n",
    "            print(f'original {vitals_names[j]} vitals array shape: {np.array(temp_vitals[j]).shape} ')\n",
    "            for i in range(len(vitals_SBS)): #go through every SBS score for each vitals metric\n",
    "                if (Filtering.checkVitals(temp_vitals[j][i], slice_size_min, vitals_names[j])): #check the data in a single window\n",
    "                    vitals_list[j].append(temp_vitals[j][i]) #append that single window data to the 2D hr,rr,spo2,bpm,bps,bpd arrays if that window's data is valid\n",
    "                else:\n",
    "                    vitals_list[j].append(flag_list) #append an array of zeros for window number i for the jth vitals metric if the data is invalid(i.e. too many NaN points)\n",
    "                    print(f'{vitals_names[j]} SBS index {i} has insufficient data, zeros appended in place') \n",
    "            print(f'final {vitals_names[j]} vitals array shape: {np.array(vitals_list[j]).shape}') #should be the number of SBS scores by the number of samples in a window\n",
    "        \n",
    "        \n",
    "        for signal, name in zip(vitals_list, vitals_names): #2D array for each vitals is input to function\n",
    "            if np.mean(np.array(signal)) != 0:\n",
    "                pearson_corr_vitals_sbs(signal, vitals_SBS, name, lead_time, slice_size_min)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
